{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analyses\n",
    "\n",
    "This code splits the df into windows of a specified length. \n",
    "The result is a list with each cell containing time_length X channels. \n",
    "The raw data contains 200ms of a prewindow and 1000ms of the test window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setup_jwlab\n",
    "from jwlab.constants import cleaned_data_filepath\n",
    "from jwlab.cluster_analysis import prep_cluster_analysis\n",
    "from jwlab.ml_prep import  average_trials_and_participants, average_trials\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy import stats\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import kurtosis, skew\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_per_window = 10\n",
    "num_sliding_windows = int(1200/ length_per_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants = [\"904\", \"905\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [\"904\", \"905\", \"906\", \"908\", \"909\", \"912\",\"913\", \"914\", \"916\", \"917\", \"919\",\\\n",
    "                \"920\",  \"923\",\"921\", \"924\", \"927\", \"928\", \"929\", \"930\", \"932\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [\"106\", \"107\", \"109\", \"111\", \"112\", \"115\", \"116\", \"117\", \"119\", \"121\", \"122\", \"120\", \"124\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [\"107\", \"109\", \"111\", \"112\", \"115\", \"116\", \"117\", \"119\", \"121\", \"122\", \"120\", \"124\",\\\n",
    "               \"904\", \"905\", \"906\", \"908\", \"909\", \"912\",\"913\", \"914\", \"916\", \"917\", \"919\",\\\n",
    "                \"920\", \"921\", \"923\", \"924\", \"927\", \"928\", \"929\", \"930\", \"932\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial count code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get trial counts\n",
    "X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1200, averaging=\"no_averaging\", length_per_window=length_per_window)\n",
    "\n",
    "\n",
    "for i in participants:\n",
    "    ps =[i]\n",
    "    X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, ps, downsample_num=1000, averaging=\"no_averaging\", length_per_window=length_per_window)\n",
    "    y[0]\n",
    "    np.amax(y[0])\n",
    "    unique, counts = np.unique(y[0], return_counts=True)\n",
    "    \n",
    "    print(i)\n",
    "    print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train raw test avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "# build raw matrix\n",
    "X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1200, averaging=\"no_average_labels\", length_per_window=length_per_window)\n",
    "# Xc, yc, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1200, averaging=\"no_averaging\", length_per_window=length_per_window)\n",
    "#Xc is just for compairson in debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(Xc))\n",
    "# print(len(Xc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# print(type(yc))\n",
    "# print(type(yc[0]))\n",
    "# print(len(yc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(df_test))\n",
    "# print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for new df with labels and ps\n",
    "num_participants = len(participants)\n",
    "num_indices = len(X[0])\n",
    "fivefold_testsize = int(.20*num_indices)\n",
    "test_indices = np.random.choice(num_indices-1, fivefold_testsize, replace=False)\n",
    "df_test = []\n",
    "df_train = []\n",
    "\n",
    "for i in range(num_sliding_windows):\n",
    "    ## will need each window\n",
    "    X[i] = X[i].reset_index()\n",
    "\n",
    "    # #create new df with these indices and removing from orig\n",
    "    df_test.append(X[i].iloc[test_indices])\n",
    "    df_train.append(X[i].drop(X[i].index[test_indices]))\n",
    "    assert(len(df_train[i]) + len(df_test[i]) == len(X[i]))\n",
    "    df_test[i] = df_test[i].drop(columns=['index'], axis=1) \n",
    "    df_train[i] = df_train[i].drop(columns=['index'], axis=1)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create training matrix:\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in range(num_sliding_windows):\n",
    "    y_train.append(df_train[i].label.values)\n",
    "    X_train.append(df_train[i].drop(columns = ['label', 'participant'], axis = 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test matrices\n",
    "X_test_t = [] # test avg trials\n",
    "y_test_t = [] \n",
    "X_test_pt = [] # test avg trials and ps\n",
    "y_test_pt = [] \n",
    "\n",
    "for i in range(num_sliding_windows):\n",
    "    X_test_t_temp, y_test_temp, ps, w = average_trials(df_test[0])\n",
    "    X_test_t.append(pd.DataFrame(X_test_t_temp))\n",
    "    y_test_t.append(y_test_temp)\n",
    "    \n",
    "    X_test_pt_temp, y_test_temp_pt, ps, w = average_trials_and_participants(df_test[0], participants)\n",
    "    X_test_pt.append(pd.DataFrame(X_test_pt_temp))\n",
    "    y_test_pt.append(y_test_temp_pt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = LinearSVC(C=1e-9, max_iter=5000)\n",
    "#model = SVC(gamma=.001, kernel = 'rbf', C = 1e-6)\n",
    "\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "for j in range(num_sliding_windows):\n",
    "        \n",
    "        model.fit(X_train[j], y_train[j])\n",
    "        \n",
    "        # validation, predict raw\n",
    "#         y_pred = model.predict(X_train[j])\n",
    "#         testScore = accuracy_score(y_train[j],y_pred)\n",
    "        \n",
    "        #predict averaged across trials\n",
    "#         y_pred = model.predict(X_test_t[j])\n",
    "#         testScore = accuracy_score(y_test_t[j],y_pred)\n",
    "        \n",
    "        # predict averaged across trials and ps\n",
    "        y_pred = model.predict(X_test_pt[j])\n",
    "        testScore = accuracy_score(y_test_pt[j],y_pred)\n",
    "\n",
    "\n",
    "        if j in results.keys(): \n",
    "            results[j].append(testScore)\n",
    "        else:\n",
    "            results[j]=[]\n",
    "            results[j].append(testScore)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008854166666666666\n",
      "[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.1875, 0.125, 0.125, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "scoreMean = []\n",
    "stdev = []\n",
    "\n",
    "for i in range(num_sliding_windows):\n",
    "    scoreMean.append(np.mean(results[i]))\n",
    "    stdev.append(np.std(results[i]))\n",
    "\n",
    "print( np.mean(scoreMean))\n",
    "print(scoreMean)\n",
    "print(stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "For raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized order of cross val, for raw data matrix\n",
    "\n",
    "X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1200, averaging=\"no_averaging\", length_per_window=length_per_window)\n",
    "\n",
    "\n",
    "num_iterations = 5\n",
    "num_folds = 5\n",
    "\n",
    "results = {}\n",
    "for i in range(num_iterations):\n",
    "    for j in range(num_sliding_windows):\n",
    "        Xfirst = X[j]\n",
    "        yfirst = y[j]\n",
    "        Xfirst['label'] = yfirst\n",
    "        Xfirst = Xfirst.sample(frac=1).reset_index(drop=True) #randomization\n",
    "        ys = Xfirst['label']\n",
    "        Xs = Xfirst.drop(columns=['label'])\n",
    "        \n",
    "        #model = SVC(gamma=.001, kernel = 'rbf', C=1)\n",
    "        model = LinearSVC(C=1, max_iter=5000)\n",
    "        cv_results = cross_validate(model, Xs, ys, cv=num_folds)\n",
    "        if j in results.keys(): \n",
    "            results[j] += cv_results['test_score'].tolist()\n",
    "        else:\n",
    "            results[j] = cv_results['test_score'].tolist()\n",
    "    print(i)\n",
    "\n",
    "    \n",
    "for i in range(num_sliding_windows):\n",
    "    assert len(results[i]) == num_iterations * num_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "For averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation with RepeatedKFold for averaged matrices\n",
    "\n",
    "#Xwin, ywin, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1000, averaging=\"average_trials\", length_per_window=length_per_window)\n",
    "#Xwin, ywin, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1000, averaging=\"average_trials_and_participants\", length_per_window=length_per_window)\n",
    "\n",
    "\n",
    "num_iterations = 3\n",
    "num_folds = 5\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "\n",
    "\n",
    "results = {}\n",
    "for j in range(num_sliding_windows):\n",
    "    X = Xwin[j]\n",
    "    y = ywin[j]\n",
    "\n",
    "    #model = SVC(gamma=.001, kernel = 'rbf', C=1e-4)\n",
    "    model = LinearSVC(C=1, max_iter=5000)\n",
    "    rkf = RepeatedKFold(n_splits=num_folds, n_repeats=num_iterations, random_state=2652124)\n",
    "    for train_index, test_index in rkf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        testScore = accuracy_score(y_test,y_pred)\n",
    "\n",
    "        if j in results.keys(): \n",
    "            results[j].append(testScore)\n",
    "        else:\n",
    "            results[j]=[]\n",
    "            results[j].append(testScore)\n",
    "    \n",
    "\n",
    "for i in range(num_sliding_windows):\n",
    "    assert len(results[i]) == num_iterations * num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreMean = []\n",
    "stdev = []\n",
    "\n",
    "for i in range(num_sliding_windows):\n",
    "    scoreMean.append(np.mean(results[i]))\n",
    "    stdev.append(np.std(results[i]))\n",
    "\n",
    "print( np.mean(scoreMean))\n",
    "print(scoreMean)\n",
    "print(stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test\n",
    "accuracy_by_guessing = [0.5] * (num_iterations * num_folds)\n",
    "pvalues = []\n",
    "for i in range(num_sliding_windows):\n",
    "    istat = stats.ttest_1samp(results[i], .5)\n",
    "    pvalues += [istat.pvalue] if istat.statistic > 0 else [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding contiguous time cluster\n",
    "valid_window = [i for i,v in enumerate(pvalues) if v <= 0.025]\n",
    "print(valid_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1200, averaging=\"no_averaging\", length_per_window=length_per_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1000, averaging=\"average_trials\", length_per_window=length_per_window)\n",
    "X, y, good_trial_count = prep_cluster_analysis(cleaned_data_filepath, participants, downsample_num=1000, averaging=\"average_trials_and_participants\", length_per_window=length_per_window)\n",
    "\n",
    "\n",
    "\n",
    "for k in range(len(X)):\n",
    "    X[k] = pd.DataFrame(data=X[k][0:,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization \n",
    "\n",
    "\n",
    "#funcs = [np.mean, np.min, np.max, np.var, skew, kurtosis]\n",
    "funcs = [np.mean, np.min, np.max, np.var]\n",
    "\n",
    "df_feats_list = []\n",
    "\n",
    "for j in range(num_sliding_windows):  \n",
    "    df_feats = X[j].apply(funcs, axis=1)\n",
    "    \n",
    "    # calc skew\n",
    "    skew_j = skew(X[j], axis = 1)\n",
    "    df_feats['skew'] = skew_j\n",
    "\n",
    "    # calc kurtosis\n",
    "    kurt_j = kurtosis(X[j], axis = 1)\n",
    "    df_feats['kurtosis'] = kurt_j\n",
    "\n",
    "\n",
    "    \n",
    "    #normalize: (x-xmin)/(max-min)\n",
    "\n",
    "    # Get column names first\n",
    "    names = df_feats.columns\n",
    "\n",
    "    normalized_df = []\n",
    "    for i in names: \n",
    "        x_array = np.array(df_feats[i])\n",
    "        normalized_X = preprocessing.normalize([x_array])\n",
    "        normalized_df.append(normalized_X)\n",
    "    \n",
    "    \n",
    "    df_1 = pd.DataFrame(np.concatenate(normalized_df))\n",
    "    df = df_1.T #transpose\n",
    "    df.columns= ['mean', 'amin', 'amax', 'var', 'skew', 'kurtosis']\n",
    "\n",
    "    \n",
    "\n",
    "    #put all windows together into a list\n",
    "    df_feats_list.append(df)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standarization: \n",
    "\n",
    "# #funcs = [np.mean, np.min, np.max, np.var, skew, kurtosis]\n",
    "# funcs = [np.mean, np.min, np.max, np.var]\n",
    "\n",
    "# df_feats_list = []\n",
    "\n",
    "# for j in range(num_sliding_windows):  \n",
    "#     df_feats = X[j].apply(funcs, axis=1)\n",
    "    \n",
    "#     # calc skew\n",
    "#     skew_j = skew(X[j], axis = 1)\n",
    "#     df_feats['skew'] = skew_j\n",
    "\n",
    "#     # calc kurtosis\n",
    "#     kurt_j = kurtosis(X[j], axis = 1)\n",
    "#     df_feats['kurtosis'] = kurt_j\n",
    "\n",
    "\n",
    "#     #standarized: (x-mean)/(stdev)\n",
    "\n",
    "#      # Get column names first\n",
    "#     names = df_feats.columns\n",
    "\n",
    "#     # Create the Scaler object\n",
    "#     scaler = preprocessing.StandardScaler()\n",
    "#     # Fit your data on the scaler object\n",
    "#     scaled_df = scaler.fit_transform(df_feats)\n",
    "#     scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "\n",
    "#     #put all windows together into a list\n",
    "#     df_feats_list.append(scaled_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross val on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_iterations = 5\n",
    "num_folds = 5\n",
    "\n",
    "results = {}\n",
    "for i in range(num_iterations):\n",
    "    for j in range(num_sliding_windows):\n",
    "        Xfirst = df_feats_list[j]\n",
    "        yfirst = y[j]\n",
    "        Xfirst['label'] = yfirst\n",
    "        Xfirst = Xfirst.sample(frac=1).reset_index(drop=True) #randomization\n",
    "        ys = Xfirst['label']\n",
    "        Xs = Xfirst.drop(columns=['label'])\n",
    "        \n",
    "        #model = SVC(gamma=.001, kernel = 'rbf', C=100)\n",
    "        model = LinearSVC(C=1e-3, max_iter=1000)\n",
    "        cv_results = cross_validate(model, Xs, ys, cv=num_folds)\n",
    "        if j in results.keys(): \n",
    "            results[j] += cv_results['test_score'].tolist()\n",
    "        else:\n",
    "            results[j] = cv_results['test_score'].tolist()\n",
    "    print(i)\n",
    "\n",
    "    \n",
    "for i in range(num_sliding_windows):\n",
    "    assert len(results[i]) == num_iterations * num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreMean = []\n",
    "stdev = []\n",
    "\n",
    "for i in range(num_sliding_windows):\n",
    "    scoreMean.append(np.mean(results[i]))\n",
    "    stdev.append(np.std(results[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scoreMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test\n",
    "accuracy_by_guessing = [0.5] * (num_iterations * num_folds)\n",
    "pvalues = []\n",
    "for i in range(num_sliding_windows):\n",
    "    istat = stats.ttest_1samp(results[i], .5)\n",
    "    pvalues += [istat.pvalue] if istat.statistic > 0 else [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding contiguous time cluster\n",
    "valid_window = [i for i,v in enumerate(pvalues) if v <= 0.025]\n",
    "print(valid_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results:\n",
    "\n",
    "x_graph = np.arange(-200,1000,length_per_window)\n",
    "y_graph = scoreMean\n",
    "stdev = np.array(stdev)\n",
    "error = stdev\n",
    "plt.plot(x_graph, y_graph, 'k-')\n",
    "plt.fill_between(x_graph, y_graph-error, y_graph+error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
